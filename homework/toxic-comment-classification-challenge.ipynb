{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/d2l-zh-pytorch-colab/blob/main/homework/toxic-comment-classification-challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "将BERT集成到您现有的训练代码中 - ModelScope版本\n",
        "使用ModelScope BERT模型进行多标签分类\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import DistilBertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"NumPy: {numpy.__version__}\")\n",
        "\n",
        "\n",
        "# 工具类\n",
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        import time\n",
        "        self.time = time\n",
        "        self.start_time = self.time.time()\n",
        "    def stop(self):\n",
        "        return self.time.time() - self.start_time\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"检测可用GPU\"\"\"\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============ 使用DistilBERT (推荐!) ============\n",
        "class DistilBERTSentimentClassifier(nn.Module):\n",
        "    \"\"\"使用DistilBERT - 多标签分类版本\"\"\"\n",
        "    def __init__(self, num_classes=6, dropout=0.1):  # 改为6个类别\n",
        "        super(DistilBERTSentimentClassifier, self).__init__()\n",
        "        from transformers import DistilBertModel\n",
        "\n",
        "        # DistilBERT: 66M参数 vs BERT: 110M参数\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # 更激进的冻结策略 - 只训练最后1层和分类器\n",
        "        for param in self.bert.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for layer in self.bert.transformer.layer[:-1]:  # 冻结除最后一层外的所有层\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 输出6个独立的sigmoid值（每个标签一个）\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # DistilBERT没有pooler_output, 使用第一个token\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]\n",
        "        # 返回原始logits，不经过sigmoid（留给损失函数处理）\n",
        "        return self.classifier(self.dropout(pooled_output))\n",
        "\n",
        "\n",
        "# ============ 修改后的训练函数 ============\n",
        "def train_bert_epoch(net, train_iter, loss, updater, device):\n",
        "    \"\"\"\n",
        "    单个epoch训练 - 添加混合精度训练\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    metric = Accumulator(3)  # 训练损失总和, 准确数, 样本数\n",
        "\n",
        "    # 使用混合精度训练\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    for _, batch in enumerate(train_iter):\n",
        "        # 解析batch\n",
        "        if len(batch) == 3:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "        else:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 混合精度前向传播\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "                l = loss(y_hat, labels)\n",
        "        else:\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            l = loss(y_hat, labels)\n",
        "\n",
        "        # 反向传播\n",
        "        if isinstance(updater, torch.optim.Optimizer):\n",
        "            updater.zero_grad()\n",
        "            if scaler is not None:\n",
        "                scaler.scale(l.sum()).backward()\n",
        "                scaler.unscale_(updater)\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "                scaler.step(updater)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                l.sum().backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "                updater.step()\n",
        "\n",
        "        # 统计\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])\n",
        "\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "\n",
        "# 训练函数\n",
        "def multilabel_accuracy(y_hat, y):\n",
        "    predictions = torch.sigmoid(y_hat) > 0.5\n",
        "    y = y.bool()\n",
        "    label_wise_acc = (predictions == y).float().mean()\n",
        "    return label_wise_acc.item()\n",
        "def train_bert_epoch(net, train_iter, loss, updater, device):\n",
        "    net.train()\n",
        "    metric = Accumulator(3)\n",
        "    for _, batch in enumerate(train_iter):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        y_hat = net(input_ids, attention_mask)\n",
        "        l = loss(y_hat, labels)\n",
        "        updater.zero_grad()\n",
        "        l.sum().backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(),max_norm=1.0)\n",
        "        updater.step()\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum(), acc * labels.shape[0],labels.shape[0])\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "def evaluate_bert_accuracy(net, data_iter, device):\n",
        "    net.eval()\n",
        "    metric = Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_bert_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices, scheduler=None):\n",
        "    \"\"\"\n",
        "    完整训练流程\n",
        "    \"\"\"\n",
        "    print('training on', devices)\n",
        "\n",
        "    if isinstance(devices, list) and len(devices) > 1:\n",
        "        # 多GPU\n",
        "        net = nn.DataParallel(net, device_ids=devices)\n",
        "\n",
        "    device = devices[0] if isinstance(devices, list) else devices\n",
        "    net = net.to(device)\n",
        "\n",
        "    timer = Timer()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练\n",
        "        train_loss, train_acc = train_bert_epoch(\n",
        "            net, train_iter, loss, trainer, device\n",
        "        )\n",
        "\n",
        "        # 验证\n",
        "        test_acc = evaluate_bert_accuracy(net, test_iter, device)\n",
        "\n",
        "        # 学习率调度\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}: '\n",
        "              f'loss {train_loss:.3f}, '\n",
        "              f'train acc {train_acc:.3f}, '\n",
        "              f'test acc {test_acc:.3f}')\n",
        "\n",
        "    print(f'Training completed in {timer.stop():.1f} sec')\n",
        "    print(f'Final: train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
        "\n",
        "\n",
        "def read_toxic_comments_real(data_dir, max_samples=None, is_train=True):\n",
        "    \"\"\"\n",
        "    读取真实的Kaggle Toxic Comment Classification数据\n",
        "    返回格式: (texts, labels, ids)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    if is_train:\n",
        "        csv_path = os.path.join(data_dir, 'train.csv')\n",
        "        print(f\"读取训练数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "        labels = df[label_columns].values.tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载训练数据: {len(texts)} 条\")\n",
        "        print(f\"标签分布: {dict(zip(label_columns, df[label_columns].sum().tolist()))}\")\n",
        "\n",
        "        return texts, labels, ids\n",
        "    else:\n",
        "        csv_path = os.path.join(data_dir, 'test.csv')\n",
        "        print(f\"读取测试数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载测试数据: {len(texts)} 条\")\n",
        "\n",
        "        return texts, None, ids\n",
        "\n",
        "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
        "    \"\"\"\n",
        "    生成Kaggle提交文件\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(\"🔮 生成预测结果...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, _ = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            # 获取logits并转换为概率\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            predictions.extend(probs)\n",
        "\n",
        "    # 创建提交DataFrame\n",
        "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
        "    })\n",
        "\n",
        "    # 保存提交文件\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"💾 提交文件已保存: {output_path}\")\n",
        "    print(f\"📊 预测统计:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
        "        print(f\"  {col}: 平均概率 {avg_prob:.4f}\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# 数据集\n",
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        if labels is not None:\n",
        "            self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "        else:\n",
        "            self.labels = torch.zeros((len(texts), 6),\n",
        "dtype=torch.float32)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return (\n",
        "            encoding['input_ids'].squeeze(),\n",
        "            encoding['attention_mask'].squeeze(),\n",
        "            self.labels[idx]\n",
        "        )\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "bEgrnIiUP6Xz"
      },
      "id": "bEgrnIiUP6Xz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============ 主要执行代码 ============\n",
        "print(\"🚀 启动 BERT多标签分类训练\")\n",
        "# 数据目录\n",
        "data_dir = 'toxic-comment'\n",
        "# 数据加载\n",
        "print(\"📊 加载真实Kaggle数据...\")\n",
        "# 为了快速训练，限制样本数（可以根据需要调整）\n",
        "train_texts, train_labels, train_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=2000, is_train=True\n",
        ")\n",
        "# 创建验证集（从训练数据中分割）\n",
        "val_split = int(len(train_texts) * 0.8)\n",
        "val_texts = train_texts[val_split:]\n",
        "val_labels = train_labels[val_split:]\n",
        "train_texts = train_texts[:val_split]\n",
        "train_labels = train_labels[:val_split]\n",
        "# 读取测试数据\n",
        "test_texts, _, test_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=None, is_train=False\n",
        ")\n",
        "# 1. 初先初始化BERT tokenizer\n",
        "# DistilBERT 和 BERT 使用相同的tokenizer，因为 DistilBERT 是从 BERT 蒸馏而来的，保留了相同的词汇表和分词方式。\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained('iic/nlp_bert_sentiment-analysis_english-base')\n",
        "print(f\"\\n📊 数据统计:\")\n",
        "print(f\"训练数据: {len(train_texts)} 条\")\n",
        "print(f\"验证数据: {len(val_texts)} 条\")\n",
        "print(f\"测试数据: {len(test_texts)} 条\")\n",
        "# 检查数据质量\n",
        "print(f\"\\n📝 数据样例:\")\n",
        "print(f\"文本长度: {len(train_texts[0])}\")\n",
        "print(f\"前100字符: {train_texts[0][:100]}\")\n",
        "print(f\"标签: {train_labels[0]}\")\n",
        "# 检查tokenizer\n",
        "sample_encoding = tokenizer(train_texts[0], max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
        "print(f\"\\n🔧 Tokenizer测试:\")\n",
        "print(f\"input_ids shape: {sample_encoding['input_ids'].shape}\")\n",
        "print(f\"attention_mask shape: {sample_encoding['attention_mask'].shape}\")\n",
        "print(f\"实际token数: {sample_encoding['attention_mask'].sum()}\")\n",
        "# 模型参数\n",
        "num_classes = 6  # 6个类别：toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
        "dropout = 0.1\n",
        "batch_size = 16  # 适配在线环境\n",
        "num_steps = 32  # 序列长度\n",
        "lr = 1e-4  # 学习率\n",
        "num_epochs = 3  # 增加到3个epoch获得更好效果\n",
        " # 使用ModelScope上实际存在的BERT模型\n",
        "\n",
        "# 创建模型\n",
        "net = DistilBERTSentimentClassifier(\n",
        "    num_classes=num_classes,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "# 创建数据加载器\n",
        "train_dataset = BERTDataset(train_texts, train_labels, tokenizer, max_length=num_steps)\n",
        "val_dataset = BERTDataset(val_texts, val_labels, tokenizer, max_length=num_steps)\n",
        "test_dataset = BERTDataset(test_texts, None, tokenizer, max_length=num_steps)\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=False)\n",
        "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
        "# 优化器 - 只训练最后一层和分类器\n",
        "trainer = AdamW([\n",
        "    {'params': [p for name, p in net.named_parameters() if 'classifier' in name], 'lr': lr * 10},  # 分类层\n",
        "    {'params': [p for name, p in net.named_parameters() if 'classifier' not in name and p.requires_grad], 'lr': lr}  # BERT层\n",
        "], weight_decay=0.01)\n",
        "# 损失函数 - 多标签分类使用BCEWithLogitsLoss\n",
        "loss = nn.BCEWithLogitsLoss(reduction=\"none\")  # 每个样本每个标签独立计算\n",
        "# 训练\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🔥 使用设备: {device}\")\n",
        "# 开始训练\n",
        "train_bert_ch13(net, train_iter, val_iter, loss, trainer, num_epochs, device, None)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 ModelScope BERT训练完成!\")\n",
        "print(\"=\"*60)\n",
        "# print(f\"✅ 模型:{model_name}\")\n",
        "print(f\"✅ 训练样本: {len(train_texts)}\")\n",
        "print(f\"✅ 验证样本: {len(val_texts)}\")\n",
        "print(f\"✅ 测试样本: {len(test_texts)}\")\n",
        "print(\"✅ 支持多标签分类\")\n",
        "print(\"✅ 混合精度训练\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "KAIasK5KP9n0"
      },
      "id": "KAIasK5KP9n0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}