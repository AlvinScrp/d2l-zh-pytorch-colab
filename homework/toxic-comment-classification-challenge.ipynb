{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/d2l-zh-pytorch-colab/blob/main/homework/toxic-comment-classification-challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "å°†BERTé›†æˆåˆ°æ‚¨ç°æœ‰çš„è®­ç»ƒä»£ç ä¸­ - ModelScopeç‰ˆæœ¬\n",
        "ä½¿ç”¨ModelScope BERTæ¨¡å‹è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import DistilBertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy\n",
        "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"NumPy: {numpy.__version__}\")\n",
        "\n",
        "\n",
        "# å·¥å…·ç±»\n",
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        import time\n",
        "        self.time = time\n",
        "        self.start_time = self.time.time()\n",
        "    def stop(self):\n",
        "        return self.time.time() - self.start_time\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"æ£€æµ‹å¯ç”¨GPU\"\"\"\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============ ä½¿ç”¨DistilBERT (æ¨è!) ============\n",
        "class DistilBERTSentimentClassifier(nn.Module):\n",
        "    \"\"\"ä½¿ç”¨DistilBERT - å¤šæ ‡ç­¾åˆ†ç±»ç‰ˆæœ¬\"\"\"\n",
        "    def __init__(self, num_classes=6, dropout=0.1):  # æ”¹ä¸º6ä¸ªç±»åˆ«\n",
        "        super(DistilBERTSentimentClassifier, self).__init__()\n",
        "        from transformers import DistilBertModel\n",
        "\n",
        "        # DistilBERT: 66Må‚æ•° vs BERT: 110Må‚æ•°\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # æ›´æ¿€è¿›çš„å†»ç»“ç­–ç•¥ - åªè®­ç»ƒæœ€å1å±‚å’Œåˆ†ç±»å™¨\n",
        "        for param in self.bert.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for layer in self.bert.transformer.layer[:-1]:  # å†»ç»“é™¤æœ€åä¸€å±‚å¤–çš„æ‰€æœ‰å±‚\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # è¾“å‡º6ä¸ªç‹¬ç«‹çš„sigmoidå€¼ï¼ˆæ¯ä¸ªæ ‡ç­¾ä¸€ä¸ªï¼‰\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # DistilBERTæ²¡æœ‰pooler_output, ä½¿ç”¨ç¬¬ä¸€ä¸ªtoken\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]\n",
        "        # è¿”å›åŸå§‹logitsï¼Œä¸ç»è¿‡sigmoidï¼ˆç•™ç»™æŸå¤±å‡½æ•°å¤„ç†ï¼‰\n",
        "        return self.classifier(self.dropout(pooled_output))\n",
        "\n",
        "\n",
        "# ============ ä¿®æ”¹åçš„è®­ç»ƒå‡½æ•° ============\n",
        "def train_bert_epoch(net, train_iter, loss, updater, device):\n",
        "    \"\"\"\n",
        "    å•ä¸ªepochè®­ç»ƒ - æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒ\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    metric = Accumulator(3)  # è®­ç»ƒæŸå¤±æ€»å’Œ, å‡†ç¡®æ•°, æ ·æœ¬æ•°\n",
        "\n",
        "    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    for _, batch in enumerate(train_iter):\n",
        "        # è§£æbatch\n",
        "        if len(batch) == 3:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "        else:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "                l = loss(y_hat, labels)\n",
        "        else:\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            l = loss(y_hat, labels)\n",
        "\n",
        "        # åå‘ä¼ æ’­\n",
        "        if isinstance(updater, torch.optim.Optimizer):\n",
        "            updater.zero_grad()\n",
        "            if scaler is not None:\n",
        "                scaler.scale(l.sum()).backward()\n",
        "                scaler.unscale_(updater)\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "                scaler.step(updater)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                l.sum().backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "                updater.step()\n",
        "\n",
        "        # ç»Ÿè®¡\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])\n",
        "\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "\n",
        "# è®­ç»ƒå‡½æ•°\n",
        "def multilabel_accuracy(y_hat, y):\n",
        "    predictions = torch.sigmoid(y_hat) > 0.5\n",
        "    y = y.bool()\n",
        "    label_wise_acc = (predictions == y).float().mean()\n",
        "    return label_wise_acc.item()\n",
        "def train_bert_epoch(net, train_iter, loss, updater, device):\n",
        "    net.train()\n",
        "    metric = Accumulator(3)\n",
        "    for _, batch in enumerate(train_iter):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        y_hat = net(input_ids, attention_mask)\n",
        "        l = loss(y_hat, labels)\n",
        "        updater.zero_grad()\n",
        "        l.sum().backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(),max_norm=1.0)\n",
        "        updater.step()\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum(), acc * labels.shape[0],labels.shape[0])\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "def evaluate_bert_accuracy(net, data_iter, device):\n",
        "    net.eval()\n",
        "    metric = Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_bert_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices, scheduler=None):\n",
        "    \"\"\"\n",
        "    å®Œæ•´è®­ç»ƒæµç¨‹\n",
        "    \"\"\"\n",
        "    print('training on', devices)\n",
        "\n",
        "    if isinstance(devices, list) and len(devices) > 1:\n",
        "        # å¤šGPU\n",
        "        net = nn.DataParallel(net, device_ids=devices)\n",
        "\n",
        "    device = devices[0] if isinstance(devices, list) else devices\n",
        "    net = net.to(device)\n",
        "\n",
        "    timer = Timer()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # è®­ç»ƒ\n",
        "        train_loss, train_acc = train_bert_epoch(\n",
        "            net, train_iter, loss, trainer, device\n",
        "        )\n",
        "\n",
        "        # éªŒè¯\n",
        "        test_acc = evaluate_bert_accuracy(net, test_iter, device)\n",
        "\n",
        "        # å­¦ä¹ ç‡è°ƒåº¦\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}: '\n",
        "              f'loss {train_loss:.3f}, '\n",
        "              f'train acc {train_acc:.3f}, '\n",
        "              f'test acc {test_acc:.3f}')\n",
        "\n",
        "    print(f'Training completed in {timer.stop():.1f} sec')\n",
        "    print(f'Final: train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
        "\n",
        "\n",
        "def read_toxic_comments_real(data_dir, max_samples=None, is_train=True):\n",
        "    \"\"\"\n",
        "    è¯»å–çœŸå®çš„Kaggle Toxic Comment Classificationæ•°æ®\n",
        "    è¿”å›æ ¼å¼: (texts, labels, ids)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    if is_train:\n",
        "        csv_path = os.path.join(data_dir, 'train.csv')\n",
        "        print(f\"è¯»å–è®­ç»ƒæ•°æ®: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "        labels = df[label_columns].values.tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"åŠ è½½è®­ç»ƒæ•°æ®: {len(texts)} æ¡\")\n",
        "        print(f\"æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(label_columns, df[label_columns].sum().tolist()))}\")\n",
        "\n",
        "        return texts, labels, ids\n",
        "    else:\n",
        "        csv_path = os.path.join(data_dir, 'test.csv')\n",
        "        print(f\"è¯»å–æµ‹è¯•æ•°æ®: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"åŠ è½½æµ‹è¯•æ•°æ®: {len(texts)} æ¡\")\n",
        "\n",
        "        return texts, None, ids\n",
        "\n",
        "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
        "    \"\"\"\n",
        "    ç”ŸæˆKaggleæäº¤æ–‡ä»¶\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(\"ğŸ”® ç”Ÿæˆé¢„æµ‹ç»“æœ...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, _ = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            # è·å–logitså¹¶è½¬æ¢ä¸ºæ¦‚ç‡\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            predictions.extend(probs)\n",
        "\n",
        "    # åˆ›å»ºæäº¤DataFrame\n",
        "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
        "    })\n",
        "\n",
        "    # ä¿å­˜æäº¤æ–‡ä»¶\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"ğŸ’¾ æäº¤æ–‡ä»¶å·²ä¿å­˜: {output_path}\")\n",
        "    print(f\"ğŸ“Š é¢„æµ‹ç»Ÿè®¡:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
        "        print(f\"  {col}: å¹³å‡æ¦‚ç‡ {avg_prob:.4f}\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# æ•°æ®é›†\n",
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        if labels is not None:\n",
        "            self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "        else:\n",
        "            self.labels = torch.zeros((len(texts), 6),\n",
        "dtype=torch.float32)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return (\n",
        "            encoding['input_ids'].squeeze(),\n",
        "            encoding['attention_mask'].squeeze(),\n",
        "            self.labels[idx]\n",
        "        )\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "bEgrnIiUP6Xz"
      },
      "id": "bEgrnIiUP6Xz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============ ä¸»è¦æ‰§è¡Œä»£ç  ============\n",
        "print(\"ğŸš€ å¯åŠ¨ BERTå¤šæ ‡ç­¾åˆ†ç±»è®­ç»ƒ\")\n",
        "# æ•°æ®ç›®å½•\n",
        "data_dir = 'toxic-comment'\n",
        "# æ•°æ®åŠ è½½\n",
        "print(\"ğŸ“Š åŠ è½½çœŸå®Kaggleæ•°æ®...\")\n",
        "# ä¸ºäº†å¿«é€Ÿè®­ç»ƒï¼Œé™åˆ¶æ ·æœ¬æ•°ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰\n",
        "train_texts, train_labels, train_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=2000, is_train=True\n",
        ")\n",
        "# åˆ›å»ºéªŒè¯é›†ï¼ˆä»è®­ç»ƒæ•°æ®ä¸­åˆ†å‰²ï¼‰\n",
        "val_split = int(len(train_texts) * 0.8)\n",
        "val_texts = train_texts[val_split:]\n",
        "val_labels = train_labels[val_split:]\n",
        "train_texts = train_texts[:val_split]\n",
        "train_labels = train_labels[:val_split]\n",
        "# è¯»å–æµ‹è¯•æ•°æ®\n",
        "test_texts, _, test_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=None, is_train=False\n",
        ")\n",
        "# 1. åˆå…ˆåˆå§‹åŒ–BERT tokenizer\n",
        "# DistilBERT å’Œ BERT ä½¿ç”¨ç›¸åŒçš„tokenizerï¼Œå› ä¸º DistilBERT æ˜¯ä» BERT è’¸é¦è€Œæ¥çš„ï¼Œä¿ç•™äº†ç›¸åŒçš„è¯æ±‡è¡¨å’Œåˆ†è¯æ–¹å¼ã€‚\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained('iic/nlp_bert_sentiment-analysis_english-base')\n",
        "print(f\"\\nğŸ“Š æ•°æ®ç»Ÿè®¡:\")\n",
        "print(f\"è®­ç»ƒæ•°æ®: {len(train_texts)} æ¡\")\n",
        "print(f\"éªŒè¯æ•°æ®: {len(val_texts)} æ¡\")\n",
        "print(f\"æµ‹è¯•æ•°æ®: {len(test_texts)} æ¡\")\n",
        "# æ£€æŸ¥æ•°æ®è´¨é‡\n",
        "print(f\"\\nğŸ“ æ•°æ®æ ·ä¾‹:\")\n",
        "print(f\"æ–‡æœ¬é•¿åº¦: {len(train_texts[0])}\")\n",
        "print(f\"å‰100å­—ç¬¦: {train_texts[0][:100]}\")\n",
        "print(f\"æ ‡ç­¾: {train_labels[0]}\")\n",
        "# æ£€æŸ¥tokenizer\n",
        "sample_encoding = tokenizer(train_texts[0], max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
        "print(f\"\\nğŸ”§ Tokenizeræµ‹è¯•:\")\n",
        "print(f\"input_ids shape: {sample_encoding['input_ids'].shape}\")\n",
        "print(f\"attention_mask shape: {sample_encoding['attention_mask'].shape}\")\n",
        "print(f\"å®é™…tokenæ•°: {sample_encoding['attention_mask'].sum()}\")\n",
        "# æ¨¡å‹å‚æ•°\n",
        "num_classes = 6  # 6ä¸ªç±»åˆ«ï¼štoxic, severe_toxic, obscene, threat, insult, identity_hate\n",
        "dropout = 0.1\n",
        "batch_size = 16  # é€‚é…åœ¨çº¿ç¯å¢ƒ\n",
        "num_steps = 32  # åºåˆ—é•¿åº¦\n",
        "lr = 1e-4  # å­¦ä¹ ç‡\n",
        "num_epochs = 3  # å¢åŠ åˆ°3ä¸ªepochè·å¾—æ›´å¥½æ•ˆæœ\n",
        " # ä½¿ç”¨ModelScopeä¸Šå®é™…å­˜åœ¨çš„BERTæ¨¡å‹\n",
        "\n",
        "# åˆ›å»ºæ¨¡å‹\n",
        "net = DistilBERTSentimentClassifier(\n",
        "    num_classes=num_classes,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "train_dataset = BERTDataset(train_texts, train_labels, tokenizer, max_length=num_steps)\n",
        "val_dataset = BERTDataset(val_texts, val_labels, tokenizer, max_length=num_steps)\n",
        "test_dataset = BERTDataset(test_texts, None, tokenizer, max_length=num_steps)\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=False)\n",
        "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
        "# ä¼˜åŒ–å™¨ - åªè®­ç»ƒæœ€åä¸€å±‚å’Œåˆ†ç±»å™¨\n",
        "trainer = AdamW([\n",
        "    {'params': [p for name, p in net.named_parameters() if 'classifier' in name], 'lr': lr * 10},  # åˆ†ç±»å±‚\n",
        "    {'params': [p for name, p in net.named_parameters() if 'classifier' not in name and p.requires_grad], 'lr': lr}  # BERTå±‚\n",
        "], weight_decay=0.01)\n",
        "# æŸå¤±å‡½æ•° - å¤šæ ‡ç­¾åˆ†ç±»ä½¿ç”¨BCEWithLogitsLoss\n",
        "loss = nn.BCEWithLogitsLoss(reduction=\"none\")  # æ¯ä¸ªæ ·æœ¬æ¯ä¸ªæ ‡ç­¾ç‹¬ç«‹è®¡ç®—\n",
        "# è®­ç»ƒ\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "train_bert_ch13(net, train_iter, val_iter, loss, trainer, num_epochs, device, None)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ ModelScope BERTè®­ç»ƒå®Œæˆ!\")\n",
        "print(\"=\"*60)\n",
        "# print(f\"âœ… æ¨¡å‹:{model_name}\")\n",
        "print(f\"âœ… è®­ç»ƒæ ·æœ¬: {len(train_texts)}\")\n",
        "print(f\"âœ… éªŒè¯æ ·æœ¬: {len(val_texts)}\")\n",
        "print(f\"âœ… æµ‹è¯•æ ·æœ¬: {len(test_texts)}\")\n",
        "print(\"âœ… æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»\")\n",
        "print(\"âœ… æ··åˆç²¾åº¦è®­ç»ƒ\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "KAIasK5KP9n0"
      },
      "id": "KAIasK5KP9n0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}