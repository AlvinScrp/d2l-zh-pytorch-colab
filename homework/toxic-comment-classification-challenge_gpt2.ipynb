{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/d2l-zh-pytorch-colab/blob/main/homework/toxic-comment-classification-challenge_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Toxic Comment Classification - gpt2 版本 多标签文本分类"
      ],
      "metadata": {
        "id": "F5NSn6ZcdRJZ"
      },
      "id": "F5NSn6ZcdRJZ"
    },
    {
      "cell_type": "code",
      "source": [
        "##屏蔽进度条，github中不支持显示，整个notebook都不显示了\n",
        "import os\n",
        "# 设置这个环境变量来禁用tqdm进度条\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import datasets\n",
        "datasets.disable_progress_bar()"
      ],
      "metadata": {
        "id": "P5LlwfuddtKu"
      },
      "id": "P5LlwfuddtKu",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "# === 1. 全局配置 ===\n",
        "URLPrefix = \"https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification\"\n",
        "DATA_DIR = Path(\"toxic-comment\")\n",
        "FILENAMES = [\"train.csv\",\"test.csv\",\"test_labels.csv\",\"sample_submission.csv\"]\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "RANDOM_STATE = 123\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# === 2. 数据准备 ===\n",
        "def prepare_csv_list():\n",
        "    # 如果toxic-comment 不存在，创建该目录\n",
        "    if not DATA_DIR.exists():\n",
        "        DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for fileName in FILENAMES:\n",
        "        URL = f\"{URLPrefix}/{fileName}\"\n",
        "        DATA_FILE =DATA_DIR/fileName\n",
        "        if not DATA_FILE.exists():\n",
        "            print(f\"⬇️ Downloading {fileName}...\")\n",
        "            with urllib.request.urlopen(URL) as r, open(DATA_FILE, \"wb\") as f:\n",
        "                f.write(r.read())\n",
        "        else:\n",
        "            print(f\"✅ already exists: {fileName} \")\n",
        "\n",
        "prepare_csv_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADMqqEdlYgRL",
        "outputId": "b5fa99ee-0e91-4ef4-c9dc-2675d9ff2d71"
      },
      "id": "ADMqqEdlYgRL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset already exists.\n",
            "✅ Dataset already exists.\n",
            "✅ Dataset already exists.\n",
            "✅ Dataset already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型GPT2ClassificationModel"
      ],
      "metadata": {
        "id": "L7UGbXCXcWPh"
      },
      "id": "L7UGbXCXcWPh"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 6):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "cknp98RDcVh5"
      },
      "id": "cknp98RDcVh5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Kaggle Toxic Comment Classification - gpt2 版本 多标签文本分类\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "\n",
        "# 设备配置\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'🔥 使用设备: {device}')\n",
        "\n",
        "# 工具类\n",
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        import time\n",
        "        self.time = time\n",
        "        self.start_time = self.time.time()\n",
        "    def stop(self):\n",
        "        return self.time.time() - self.start_time\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"检测可用GPU\"\"\"\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"文本预处理和词汇表构建器\"\"\"\n",
        "\n",
        "    def __init__(self, max_vocab_size=50000, max_seq_length=128, min_freq=2):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.min_freq = min_freq\n",
        "        self.vocab = None\n",
        "        self.word_to_idx = None\n",
        "        self.idx_to_word = None\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"清理文本\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # 转换为小写\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # 移除特殊字符，保留字母数字和基本标点\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:]', ' ', text)\n",
        "\n",
        "        # 移除多余空格\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"构建词汇表\"\"\"\n",
        "        print(\"📝 构建词汇表...\")\n",
        "\n",
        "        # 清理文本并分词\n",
        "        word_counts = Counter()\n",
        "        for text in tqdm(texts, desc=\"处理文本\"):\n",
        "            cleaned_text = self.clean_text(text)\n",
        "            words = cleaned_text.split()\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # 构建词汇表：保留高频词\n",
        "        vocab_items = [word for word, count in word_counts.most_common(self.max_vocab_size-2)\n",
        "                      if count >= self.min_freq]\n",
        "\n",
        "        # 添加特殊标记\n",
        "        self.vocab = ['<PAD>', '<UNK>'] + vocab_items\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "\n",
        "        print(f\"词汇表大小: {len(self.vocab)}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def text_to_sequence(self, text):\n",
        "        \"\"\"文本转换为序列\"\"\"\n",
        "        cleaned_text = self.clean_text(text)\n",
        "        words = cleaned_text.split()\n",
        "\n",
        "        # 转换为索引\n",
        "        indices = []\n",
        "        for word in words[:self.max_seq_length]:\n",
        "            idx = self.word_to_idx.get(word, 1)  # 1 是 <UNK> 的索引\n",
        "            indices.append(idx)\n",
        "\n",
        "        # 填充到固定长度\n",
        "        if len(indices) < self.max_seq_length:\n",
        "            indices.extend([0] * (self.max_seq_length - len(indices)))\n",
        "\n",
        "        return indices[:self.max_seq_length]\n",
        "\n",
        "class ToxicCommentDataset(Dataset):\n",
        "    \"\"\"有毒评论数据集\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, preprocessor):\n",
        "        self.texts = texts\n",
        "        self.labels = labels if labels is not None else [[0]*6]*len(texts)\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        sequence = self.preprocessor.text_to_sequence(text)\n",
        "\n",
        "        # 创建attention mask（非零位置为1）\n",
        "        attention_mask = [1 if token != 0 else 0 for token in sequence]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(sequence, dtype=torch.long),\n",
        "            torch.tensor(attention_mask, dtype=torch.long),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        )\n",
        "\n",
        "def multilabel_accuracy(y_hat, y):\n",
        "    \"\"\"多标签分类准确率\"\"\"\n",
        "    predictions = torch.sigmoid(y_hat) > 0.5\n",
        "    y = y.bool()\n",
        "    label_wise_acc = (predictions == y).float().mean()\n",
        "    return label_wise_acc.item()\n",
        "\n",
        "def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None):\n",
        "    \"\"\"\n",
        "    单个epoch训练 - 混合精度训练 + 学习率调度\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    metric = Accumulator(3)  # 训练损失总和, 准确数, 样本数\n",
        "\n",
        "    # 使用混合精度训练\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    for _, batch in enumerate(train_iter):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device, non_blocking=True)\n",
        "        attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        # 混合精度前向传播\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "                l = loss(y_hat, labels)\n",
        "        else:\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            l = loss(y_hat, labels)\n",
        "\n",
        "        updater.zero_grad()\n",
        "\n",
        "        # 混合精度反向传播\n",
        "        if scaler is not None:\n",
        "            scaler.scale(l.sum()).backward()\n",
        "            scaler.unscale_(updater)\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            scaler.step(updater)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            l.sum().backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            updater.step()\n",
        "\n",
        "        # 学习率调度（OneCycleLR需要在每个batch后调用）\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])\n",
        "\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "def evaluate_gpt2_accuracy(net, data_iter, device):\n",
        "    net.eval()\n",
        "    metric = Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # 使用混合精度推理\n",
        "            if device.type == 'cuda':\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    y_hat = net(input_ids, attention_mask)\n",
        "            else:\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_gpt2_model(net, train_iter, test_iter, loss, trainer, num_epochs, devices, scheduler=None):\n",
        "    \"\"\"\n",
        "    完整训练流程\n",
        "    \"\"\"\n",
        "    print('training on', devices)\n",
        "\n",
        "    if isinstance(devices, list) and len(devices) > 1:\n",
        "        # 多GPU\n",
        "        net = nn.DataParallel(net, device_ids=devices)\n",
        "\n",
        "    device = devices[0] if isinstance(devices, list) else devices\n",
        "    net = net.to(device)\n",
        "\n",
        "    timer = Timer()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练 - 传递调度器\n",
        "        train_loss, train_acc = train_gpt2_epoch(\n",
        "            net, train_iter, loss, trainer, device, scheduler\n",
        "        )\n",
        "\n",
        "        # 验证\n",
        "        test_acc = evaluate_gpt2_accuracy(net, test_iter, device)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}: '\n",
        "              f'loss {train_loss:.3f}, '\n",
        "              f'train acc {train_acc:.3f}, '\n",
        "              f'test acc {test_acc:.3f}, '\n",
        "              f'lr {trainer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    print(f'Training completed in {timer.stop():.1f} sec')\n",
        "    print(f'Final: train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
        "\n",
        "def read_toxic_comments_real(data_dir, max_samples=None, is_train=True):\n",
        "    \"\"\"\n",
        "    读取真实的Kaggle Toxic Comment Classification数据\n",
        "    返回格式: (texts, labels, ids)\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        csv_path = os.path.join(data_dir, 'train.csv')\n",
        "        print(f\"读取训练数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "        labels = df[label_columns].values.tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载训练数据: {len(texts)} 条\")\n",
        "        print(f\"标签分布: {dict(zip(label_columns, df[label_columns].sum().tolist()))}\")\n",
        "\n",
        "        return texts, labels, ids\n",
        "    else:\n",
        "        csv_path = os.path.join(data_dir, 'test.csv')\n",
        "        print(f\"读取测试数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载测试数据: {len(texts)} 条\")\n",
        "\n",
        "        return texts, None, ids\n",
        "\n",
        "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
        "    \"\"\"\n",
        "    生成Kaggle提交文件\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(\"🔮 生成预测结果...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, _ = batch\n",
        "            input_ids = input_ids.to(device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "            # 使用混合精度推理\n",
        "            if device.type == 'cuda':\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(input_ids, attention_mask)\n",
        "            else:\n",
        "                logits = model(input_ids, attention_mask)\n",
        "\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            predictions.extend(probs)\n",
        "\n",
        "    # 创建提交DataFrame\n",
        "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
        "    })\n",
        "\n",
        "    # 保存提交文件\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"💾 提交文件已保存: {output_path}\")\n",
        "    print(f\"📊 预测统计:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
        "        print(f\"  {col}: 平均概率 {avg_prob:.4f}\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# ============ 主要执行代码 ============\n",
        "print(\"🚀 启动双向GPT2多标签分类训练\")\n",
        "\n",
        "# 数据目录\n",
        "data_dir = 'toxic-comment'\n",
        "prepare_csv_list()\n",
        "\n",
        "# 数据加载\n",
        "print(\"📊 加载真实Kaggle数据...\")\n",
        "\n",
        "# 为了快速训练，限制样本数（可以根据需要调整）\n",
        "train_texts, train_labels, train_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=None, is_train=True\n",
        ")\n",
        "\n",
        "# 创建验证集（从训练数据中分割）\n",
        "val_split = int(len(train_texts) * 0.8)\n",
        "val_texts = train_texts[val_split:]\n",
        "val_labels = train_labels[val_split:]\n",
        "train_texts = train_texts[:val_split]\n",
        "train_labels = train_labels[:val_split]\n",
        "\n",
        "# 读取测试数据\n",
        "test_texts, _, test_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=None, is_train=False\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 数据统计:\")\n",
        "print(f\"训练数据: {len(train_texts)} 条\")\n",
        "print(f\"验证数据: {len(val_texts)} 条\")\n",
        "print(f\"测试数据: {len(test_texts)} 条\")\n",
        "\n",
        "# 检查数据质量\n",
        "print(f\"\\n📝 数据样例:\")\n",
        "print(f\"文本长度: {len(train_texts[0])}\")\n",
        "print(f\"前100字符: {train_texts[0][:100]}\")\n",
        "print(f\"标签: {train_labels[0]}\")\n",
        "\n",
        "# 创建文本预处理器和词汇表 - 优化参数以提升速度\n",
        "preprocessor = TextPreprocessor(\n",
        "    max_vocab_size=20000,  # 从50000降到20000\n",
        "    max_seq_length=64,     # 从128降到64\n",
        "    min_freq=3             # 提高最小频率阈值\n",
        ")\n",
        "preprocessor.build_vocab(train_texts)\n",
        "\n",
        "print(f\"\\n🔧 预处理器测试:\")\n",
        "sample_sequence = preprocessor.text_to_sequence(train_texts[0])\n",
        "print(f\"序列长度: {len(sample_sequence)}\")\n",
        "print(f\"非零token数: {sum(1 for x in sample_sequence if x != 0)}\")\n",
        "\n",
        "# 模型参数 - 优化以提升训练速度\n",
        "num_classes = 6  # 6个类别：toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
        "dropout = 0.3\n",
        "# 根据GPU情况自动调整批次大小\n",
        "batch_size = 32 if torch.cuda.is_available() else 16\n",
        "num_steps = 128   # 序列长度（从128降到64）\n",
        "lr = 2e-3        # 提高学习率以加快收敛\n",
        "num_epochs = 3   # 训练轮数\n",
        "\n",
        "# 创建双向LSTM模型 - 优化结构以提升速度\n",
        "net = GPT2ClassificationModel()\n",
        "\n",
        "print(f\"模型参数数量: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "\n",
        "# 创建数据加载器\n",
        "train_dataset = ToxicCommentDataset(train_texts, train_labels, preprocessor)\n",
        "val_dataset = ToxicCommentDataset(val_texts, val_labels, preprocessor)\n",
        "test_dataset = ToxicCommentDataset(test_texts, None, preprocessor)\n",
        "\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                          num_workers=4, pin_memory=True, persistent_workers=True)\n",
        "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                       num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                        num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "# 优化器和学习率调度器\n",
        "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "# 添加学习率调度器以提升训练效果\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    trainer,\n",
        "    max_lr=lr * 5,  # 最大学习率\n",
        "    steps_per_epoch=len(train_iter),\n",
        "    epochs=num_epochs,\n",
        "    pct_start=0.3  # 前30%时间用于升温\n",
        ")\n",
        "\n",
        "# 损失函数 - 多标签分类使用BCEWithLogitsLoss\n",
        "loss = nn.BCEWithLogitsLoss(reduction=\"none\")  # 每个样本每个标签独立计算\n",
        "\n",
        "# 训练\n",
        "print(f\"🔥 使用设备: {device}\")\n",
        "\n",
        "# 开始训练 - 使用学习率调度器\n",
        "train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 Huggingface GPT2训练完成!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"✅ 训练样本: {len(train_texts)}\")\n",
        "print(f\"✅ 验证样本: {len(val_texts)}\")\n",
        "print(f\"✅ 测试样本: {len(test_texts)}\")\n",
        "print(\"✅ 支持多标签分类\")\n",
        "print(\"✅ 词汇表大小: {:,}\".format(len(preprocessor.vocab)))\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "35KighgnXCUo"
      },
      "id": "35KighgnXCUo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成提交文件\n",
        "submission_path = os.path.join(data_dir, 'submission.csv')\n",
        "submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)\n",
        "\n",
        "print(\"\\n🎉 训练和预测完成!\")\n",
        "print(f\"✅ 提交文件: {submission_path}\")"
      ],
      "metadata": {
        "id": "mlNsQ60KXIEE"
      },
      "id": "mlNsQ60KXIEE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}