{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/d2l-zh-pytorch-colab/blob/main/homework/toxic-comment-classification-challenge_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Toxic Comment Classification - gpt2 版本 多标签文本分类"
      ],
      "metadata": {
        "id": "F5NSn6ZcdRJZ"
      },
      "id": "F5NSn6ZcdRJZ"
    },
    {
      "cell_type": "code",
      "source": [
        "##屏蔽进度条，github中不支持显示，整个notebook都不显示了\n",
        "import os\n",
        "# 设置这个环境变量来禁用tqdm进度条\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import datasets\n",
        "datasets.disable_progress_bar()"
      ],
      "metadata": {
        "id": "P5LlwfuddtKu"
      },
      "id": "P5LlwfuddtKu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 数据集"
      ],
      "metadata": {
        "id": "VlFGnJIN8lC4"
      },
      "id": "VlFGnJIN8lC4"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "# === 1. 全局配置 ===\n",
        "URLPrefix = \"https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification\"\n",
        "data_dir = 'toxic-comment'\n",
        "DATA_DIR = Path(data_dir)\n",
        "FILENAMES = [\"train.csv\",\"test.csv\",\"test_labels.csv\",\"sample_submission.csv\"]\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "RANDOM_STATE = 123\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# === 2. 数据准备 ===\n",
        "def prepare_csv_list():\n",
        "    # 如果toxic-comment 不存在，创建该目录\n",
        "    if not DATA_DIR.exists():\n",
        "        DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for fileName in FILENAMES:\n",
        "        URL = f\"{URLPrefix}/{fileName}\"\n",
        "        DATA_FILE =DATA_DIR/fileName\n",
        "        if not DATA_FILE.exists():\n",
        "            print(f\"⬇️ Downloading {fileName}...\")\n",
        "            with urllib.request.urlopen(URL) as r, open(DATA_FILE, \"wb\") as f:\n",
        "                f.write(r.read())\n",
        "        else:\n",
        "            print(f\"✅ already exists: {fileName} \")\n",
        "\n",
        "prepare_csv_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADMqqEdlYgRL",
        "outputId": "3016e445-61cd-4ede-8984-f6d3764e5e4a"
      },
      "id": "ADMqqEdlYgRL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ already exists: train.csv \n",
            "✅ already exists: test.csv \n",
            "✅ already exists: test_labels.csv \n",
            "✅ already exists: sample_submission.csv \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"使用 GPT2Tokenizer 的文本预处理器\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name=\"gpt2\",\n",
        "                 max_seq_length=128,\n",
        "                 add_special_tokens=True,\n",
        "                 padding=True,\n",
        "                 truncation=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_name (str): 使用的预训练 GPT2 分词器名称（如 \"gpt2\", \"gpt2-medium\"）\n",
        "            max_seq_length (int): 最大序列长度\n",
        "            add_special_tokens (bool): 是否添加特殊标记（如 BOS/EOS）\n",
        "            padding (bool): 是否自动填充\n",
        "            truncation (bool): 是否截断超长文本\n",
        "        \"\"\"\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.add_special_tokens = add_special_tokens\n",
        "        self.padding = padding\n",
        "        self.truncation = truncation\n",
        "\n",
        "        # ✅ 初始化 GPT2 分词器\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        # GPT-2 默认没有 pad_token，需要手动设置\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(f\"✅ GPT2Tokenizer 已加载: {model_name}\")\n",
        "        print(f\"词表大小: {len(self.tokenizer)}\")\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"可选的文本清理（保留基础清洗逻辑）\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"兼容旧接口（GPT2Tokenizer 自带词汇表，无需手动构建）\"\"\"\n",
        "        print(\"⚙️ 使用 GPT2Tokenizer 自带词汇表，无需手动构建。\")\n",
        "        return list(self.tokenizer.get_vocab().keys())\n",
        "\n",
        "    def text_to_sequence(self, text):\n",
        "        \"\"\"文本转为 GPT-2 Token ID 序列\"\"\"\n",
        "        cleaned_text = self.clean_text(text)\n",
        "        encoding = self.tokenizer(\n",
        "            cleaned_text,\n",
        "            add_special_tokens=self.add_special_tokens,\n",
        "            max_length=self.max_seq_length,\n",
        "            padding='max_length' if self.padding else False,\n",
        "            truncation=self.truncation,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        return encoding[\"input_ids\"]\n",
        "\n",
        "    def batch_encode(self, texts):\n",
        "        \"\"\"批量文本编码\"\"\"\n",
        "        cleaned_texts = [self.clean_text(t) for t in texts]\n",
        "        encodings = self.tokenizer(\n",
        "            cleaned_texts,\n",
        "            add_special_tokens=self.add_special_tokens,\n",
        "            max_length=self.max_seq_length,\n",
        "            padding='max_length' if self.padding else False,\n",
        "            truncation=self.truncation,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        return encodings[\"input_ids\"]\n",
        "\n",
        "\n",
        "class ToxicCommentDataset(Dataset):\n",
        "    \"\"\"有毒评论数据集\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, preprocessor):\n",
        "        self.texts = texts\n",
        "        # self.labels = labels if labels is not None else [[0]*6]*len(texts)\n",
        "        self.labels = labels if labels is not None else [[0]*6 for _ in range(len(texts))]\n",
        "\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        sequence = self.preprocessor.text_to_sequence(text)\n",
        "\n",
        "        if not sequence or len(sequence) == 0:\n",
        "            sequence = [self.preprocessor.tokenizer.pad_token_id] * self.preprocessor.max_seq_length\n",
        "\n",
        "\n",
        "        # 创建attention mask（非零位置为1）\n",
        "        attention_mask = [1 if token != 0 else 0 for token in sequence]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(sequence, dtype=torch.long),\n",
        "            torch.tensor(attention_mask, dtype=torch.long),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        )\n",
        "\n",
        "\n",
        "def read_toxic_comments_real(data_dir, max_samples=None, is_train=True):\n",
        "    \"\"\"\n",
        "    读取真实的Kaggle Toxic Comment Classification数据\n",
        "    返回格式: (texts, labels, ids)\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        csv_path = os.path.join(data_dir, 'train.csv')\n",
        "        print(f\"读取训练数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "        labels = df[label_columns].values.tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载训练数据: {len(texts)} 条\")\n",
        "        print(f\"标签分布: {dict(zip(label_columns, df[label_columns].sum().tolist()))}\")\n",
        "\n",
        "        return texts, labels, ids\n",
        "    else:\n",
        "        csv_path = os.path.join(data_dir, 'test.csv')\n",
        "        print(f\"读取测试数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载测试数据: {len(texts)} 条\")\n",
        "\n",
        "        return texts, None, ids\n",
        "\n",
        "def create_dataloaders(data_dir,batch_size):\n",
        "# 数据目录\n",
        "    prepare_csv_list()\n",
        "\n",
        "    # 数据加载\n",
        "    print(\"📊 加载真实Kaggle数据...\")\n",
        "\n",
        "    # 为了快速训练，限制样本数（可以根据需要调整）\n",
        "    train_texts, train_labels, train_ids = read_toxic_comments_real(\n",
        "        data_dir, max_samples=None, is_train=True\n",
        "    )\n",
        "\n",
        "    # 创建验证集（从训练数据中分割）\n",
        "    val_split = int(len(train_texts) * 0.8)\n",
        "    val_texts = train_texts[val_split:]\n",
        "    val_labels = train_labels[val_split:]\n",
        "    train_texts = train_texts[:val_split]\n",
        "    train_labels = train_labels[:val_split]\n",
        "\n",
        "    # 读取测试数据\n",
        "    test_texts, _, test_ids = read_toxic_comments_real(\n",
        "        data_dir, max_samples=None, is_train=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n📊 数据统计:\")\n",
        "    print(f\"训练数据: {len(train_texts)} 条\")\n",
        "    print(f\"验证数据: {len(val_texts)} 条\")\n",
        "    print(f\"测试数据: {len(test_texts)} 条\")\n",
        "\n",
        "    # 检查数据质量\n",
        "    print(f\"\\n📝 数据样例:\")\n",
        "    print(f\"文本长度: {len(train_texts[0])}\")\n",
        "    print(f\"前100字符: {train_texts[0][:100]}\")\n",
        "    print(f\"标签: {train_labels[0]}\")\n",
        "\n",
        "    preprocessor = TextPreprocessor()\n",
        "\n",
        "    print(f\"\\n🔧 预处理器测试:\")\n",
        "    sample_sequence = preprocessor.text_to_sequence(train_texts[0])\n",
        "    print(f\"序列长度: {len(sample_sequence)}\")\n",
        "    print(f\"非零token数: {sum(1 for x in sample_sequence if x != 0)}\")\n",
        "\n",
        "\n",
        "\n",
        "    # 创建数据加载器\n",
        "    train_dataset = ToxicCommentDataset(train_texts, train_labels, preprocessor)\n",
        "    val_dataset = ToxicCommentDataset(val_texts, val_labels, preprocessor)\n",
        "    test_dataset = ToxicCommentDataset(test_texts, None, preprocessor)\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                              num_workers=4, pin_memory=True, persistent_workers=True)\n",
        "    val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                           num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                            num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "    return train_iter, val_iter, test_iter,test_ids"
      ],
      "metadata": {
        "id": "3RpIm1GN8no_"
      },
      "id": "3RpIm1GN8no_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型与训练方法"
      ],
      "metadata": {
        "id": "L7UGbXCXcWPh"
      },
      "id": "L7UGbXCXcWPh"
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Kaggle Toxic Comment Classification - gpt2 版本 多标签文本分类\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "\n",
        "\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 6):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits\n",
        "\n",
        "# 工具类\n",
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        import time\n",
        "        self.time = time\n",
        "        self.start_time = self.time.time()\n",
        "    def stop(self):\n",
        "        return self.time.time() - self.start_time\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"检测可用GPU\"\"\"\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "def multilabel_accuracy(y_hat, y):\n",
        "    \"\"\"多标签分类准确率\"\"\"\n",
        "    predictions = torch.sigmoid(y_hat) > 0.5\n",
        "    y = y.bool()\n",
        "    label_wise_acc = (predictions == y).float().mean()\n",
        "    return label_wise_acc.item()\n",
        "\n",
        "def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None,progress_bar=None):\n",
        "    \"\"\"\n",
        "    单个epoch训练 - 混合精度训练 + 学习率调度\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    metric = Accumulator(3)  # 训练损失总和, 准确数, 样本数\n",
        "\n",
        "    # 使用混合精度训练\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _, batch in enumerate(train_iter):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device, non_blocking=True)\n",
        "        attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        # 混合精度前向传播\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "                l = loss(y_hat, labels)\n",
        "        else:\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            l = loss(y_hat, labels)\n",
        "\n",
        "        updater.zero_grad()\n",
        "\n",
        "        # 混合精度反向传播\n",
        "        if scaler is not None:\n",
        "            scaler.scale(l.sum()).backward()\n",
        "            scaler.unscale_(updater)\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            scaler.step(updater)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            l.sum().backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            updater.step()\n",
        "\n",
        "        # 学习率调度（OneCycleLR需要在每个batch后调用）\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])\n",
        "\n",
        "        cost = time.time() - start_time\n",
        "        if progress_bar is not None:\n",
        "            progress_bar.set_postfix({\"Cost\": f\"{cost:.2f}s\"})\n",
        "\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "def evaluate_gpt2_accuracy(net, data_iter, device):\n",
        "    net.eval()\n",
        "    metric = Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # 使用混合精度推理\n",
        "            if device.type == 'cuda':\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    y_hat = net(input_ids, attention_mask)\n",
        "            else:\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices, scheduler=None):\n",
        "    \"\"\"\n",
        "    完整训练流程\n",
        "    \"\"\"\n",
        "    print('training on', devices)\n",
        "\n",
        "    if isinstance(devices, list) and len(devices) > 1:\n",
        "        # 多GPU\n",
        "        net = nn.DataParallel(net, device_ids=devices)\n",
        "\n",
        "    device = devices[0] if isinstance(devices, list) else devices\n",
        "    net = net.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_iter_tqdm = tqdm(train_iter,\n",
        "                            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                            bar_format=\"{desc}: {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        # 训练\n",
        "        train_loss, train_acc = train_gpt2_epoch(\n",
        "            net, train_iter_tqdm, loss, trainer, device, scheduler,train_iter_tqdm\n",
        "        )\n",
        "\n",
        "        # 验证\n",
        "        val_acc = evaluate_gpt2_accuracy(net, val_iter, device)\n",
        "\n",
        "        tqdm.write(f'Epoch {epoch + 1}: '\n",
        "              f'loss {train_loss:.3f}, '\n",
        "              f'train acc {train_acc:.3f}, '\n",
        "              f'val acc {val_acc:.3f}, '\n",
        "              f'lr {trainer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    print(f'Training completed in {timer.stop():.1f} sec')\n",
        "    print(f'Final: train acc {train_acc:.3f}, val acc {val_acc:.3f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "35KighgnXCUo",
        "outputId": "57c7b8d2-11c2-41ef-d2cb-18c6e87b0e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "35KighgnXCUo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch版本: 2.8.0+cu126\n",
            "NumPy: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练"
      ],
      "metadata": {
        "id": "2NSDANHe0eY1"
      },
      "id": "2NSDANHe0eY1"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 主要执行代码 ============\n",
        "print(\"🚀 启动双向GPT2多标签分类训练\")\n",
        "\n",
        "# 设备配置\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'🔥 使用设备: {device}')\n",
        "\n",
        "# 模型参数 - 优化以提升训练速度\n",
        "num_classes = 6  # 6个类别：toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
        "# 根据GPU情况自动调整批次大小\n",
        "batch_size = 32 if torch.cuda.is_available() else 16\n",
        "num_steps = 128   # 序列长度（从128降到64）\n",
        "lr = 2e-3        # 提高学习率以加快收敛\n",
        "num_epochs = 3   # 训练轮数\n",
        "\n",
        "train_iter, val_iter, test_iter, test_ids =  create_dataloaders(data_dir,batch_size)\n",
        "\n",
        "net = GPT2ClassificationModel()\n",
        "print(f\"模型参数数量: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "net.to(device)\n",
        "\n",
        "# --- 支持：冻结 GPT2 模型的参数，或者只训练某几层 ---\n",
        "for param in net.gpt2.parameters():\n",
        "    param.requires_grad = False\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 优化器和学习率调度器\n",
        "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "# 添加学习率调度器以提升训练效果\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    trainer,\n",
        "    max_lr=lr * 5,  # 最大学习率\n",
        "    steps_per_epoch=len(train_iter),\n",
        "    epochs=num_epochs,\n",
        "    pct_start=0.3  # 前30%时间用于升温\n",
        ")\n",
        "\n",
        "# 损失函数 - 多标签分类使用BCEWithLogitsLoss\n",
        "loss = nn.BCEWithLogitsLoss(reduction=\"none\")  # 每个样本每个标签独立计算\n",
        "\n",
        "\n",
        "# 开始训练 - 使用学习率调度器\n",
        "# train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 Huggingface GPT2 多标签分类 训练完成!\")\n"
      ],
      "metadata": {
        "id": "pLrF7iylBnVP",
        "outputId": "642b68a6-5627-48b1-d806-449e2767b734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pLrF7iylBnVP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 启动双向GPT2多标签分类训练\n",
            "🔥 使用设备: cpu\n",
            "✅ already exists: train.csv \n",
            "✅ already exists: test.csv \n",
            "✅ already exists: test_labels.csv \n",
            "✅ already exists: sample_submission.csv \n",
            "📊 加载真实Kaggle数据...\n",
            "读取训练数据: toxic-comment/train.csv\n",
            "加载训练数据: 159571 条\n",
            "标签分布: {'toxic': 15294, 'severe_toxic': 1595, 'obscene': 8449, 'threat': 478, 'insult': 7877, 'identity_hate': 1405}\n",
            "读取测试数据: toxic-comment/test.csv\n",
            "加载测试数据: 153164 条\n",
            "\n",
            "📊 数据统计:\n",
            "训练数据: 127656 条\n",
            "验证数据: 31915 条\n",
            "测试数据: 153164 条\n",
            "\n",
            "📝 数据样例:\n",
            "文本长度: 264\n",
            "前100字符: Explanation\n",
            "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't \n",
            "标签: [0, 0, 0, 0, 0, 0]\n",
            "✅ GPT2Tokenizer 已加载: gpt2\n",
            "词表大小: 50257\n",
            "\n",
            "🔧 预处理器测试:\n",
            "序列长度: 128\n",
            "非零token数: 128\n",
            "模型参数数量: 124,444,422\n",
            "\n",
            "============================================================\n",
            "🎉 Huggingface GPT2 多标签分类 训练完成!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 预测并生成提交文件"
      ],
      "metadata": {
        "id": "6JSobU6988Kz"
      },
      "id": "6JSobU6988Kz"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tqdm from tqdm\n",
        "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
        "    \"\"\"\n",
        "    生成Kaggle提交文件\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(\"🔮 生成预测结果...\")\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        test_loader_tqdm = tqdm(test_loader,\n",
        "                            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                            bar_format=\"{desc}: {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        for batch in test_loader_tqdm:\n",
        "            try:\n",
        "                input_ids, attention_mask, _ = batch\n",
        "                input_ids = input_ids.to(device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "                # 使用混合精度推理\n",
        "                if device.type == 'cuda':\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        logits = model(input_ids, attention_mask)\n",
        "                else:\n",
        "                    logits = model(input_ids, attention_mask)\n",
        "\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                predictions.extend(probs)\n",
        "                cost = time.time() - start_time\n",
        "                test_loader_tqdm.set_postfix({\"Cost\": f\"{cost:.2f}s\"})\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 在 batch {i} 报错：{repr(e)}\")\n",
        "                print(f\"batch 内容信息:\")\n",
        "                for j, item in enumerate(batch):\n",
        "                    if torch.is_tensor(item):\n",
        "                        print(f\"  tensor[{j}] -> shape: {item.shape}, dtype: {item.dtype}\")\n",
        "                    else:\n",
        "                        print(f\"  非tensor[{j}]: {type(item)}\")\n",
        "                raise e\n",
        "\n",
        "\n",
        "    # 创建提交DataFrame\n",
        "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
        "    })\n",
        "\n",
        "    # 保存提交文件\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"💾 提交文件已保存: {output_path}\")\n",
        "    print(f\"📊 预测统计:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
        "        print(f\"  {col}: 平均概率 {avg_prob:.4f}\")\n",
        "\n",
        "    return submission_df"
      ],
      "metadata": {
        "id": "6HYfDedW85sQ"
      },
      "id": "6HYfDedW85sQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成提交文件\n",
        "submission_path = os.path.join(data_dir, 'submission.csv')\n",
        "submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)\n",
        "\n",
        "print(\"\\n🎉 训练和预测完成!\")\n",
        "print(f\"✅ 提交文件: {submission_path}\")"
      ],
      "metadata": {
        "id": "mlNsQ60KXIEE",
        "outputId": "bd6f6832-c216-4969-8f99-a14b92db689b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mlNsQ60KXIEE",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔮 生成预测结果...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}