{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/d2l-zh-pytorch-colab/blob/main/homework/toxic-comment-classification-challenge_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Kaggle Toxic Comment Classification - BiLSTM 版本\n",
        "使用双向LSTM编码器替换DistilBERT实现多标签文本分类\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "\n",
        "# 设备配置\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'🔥 使用设备: {device}')\n",
        "\n",
        "# 工具类\n",
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        import time\n",
        "        self.time = time\n",
        "        self.start_time = self.time.time()\n",
        "    def stop(self):\n",
        "        return self.time.time() - self.start_time\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"检测可用GPU\"\"\"\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class BiLSTMSentimentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    双向LSTM多标签情感分类器\n",
        "    用于替换DistilBERT实现有毒评论分类\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, num_layers=2,\n",
        "                 num_classes=6, dropout=0.3, pretrained_embeddings=None):\n",
        "        super(BiLSTMSentimentClassifier, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # 嵌入层\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "        # 双向LSTM层\n",
        "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        # 注意力机制\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # 分类器\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def attention_net(self, lstm_output):\n",
        "        \"\"\"\n",
        "        注意力机制：计算加权平均的上下文向量\n",
        "        \"\"\"\n",
        "        # lstm_output: (batch_size, seq_len, hidden_dim*2)\n",
        "        attention_weights = torch.tanh(self.attention(lstm_output))  # (batch_size, seq_len, 1)\n",
        "        attention_weights = torch.softmax(attention_weights.squeeze(-1), dim=1)  # (batch_size, seq_len)\n",
        "\n",
        "        # 加权求和\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), lstm_output)  # (batch_size, 1, hidden_dim*2)\n",
        "        context_vector = context_vector.squeeze(1)  # (batch_size, hidden_dim*2)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        attention_mask: (batch_size, seq_len) - 可选，用于掩码\n",
        "        \"\"\"\n",
        "        # 嵌入\n",
        "        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # 双向LSTM\n",
        "        lstm_output, (hidden, cell) = self.bilstm(embedded)  # lstm_output: (batch_size, seq_len, hidden_dim*2)\n",
        "\n",
        "        # 注意力机制\n",
        "        context_vector, attention_weights = self.attention_net(lstm_output)\n",
        "\n",
        "        # 分类\n",
        "        output = self.dropout(context_vector)\n",
        "        logits = self.classifier(output)  # (batch_size, num_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"文本预处理和词汇表构建器\"\"\"\n",
        "\n",
        "    def __init__(self, max_vocab_size=50000, max_seq_length=128, min_freq=2):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.min_freq = min_freq\n",
        "        self.vocab = None\n",
        "        self.word_to_idx = None\n",
        "        self.idx_to_word = None\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"清理文本\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # 转换为小写\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # 移除特殊字符，保留字母数字和基本标点\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:]', ' ', text)\n",
        "\n",
        "        # 移除多余空格\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"构建词汇表\"\"\"\n",
        "        print(\"📝 构建词汇表...\")\n",
        "\n",
        "        # 清理文本并分词\n",
        "        word_counts = Counter()\n",
        "        for text in tqdm(texts, desc=\"处理文本\"):\n",
        "            cleaned_text = self.clean_text(text)\n",
        "            words = cleaned_text.split()\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # 构建词汇表：保留高频词\n",
        "        vocab_items = [word for word, count in word_counts.most_common(self.max_vocab_size-2)\n",
        "                      if count >= self.min_freq]\n",
        "\n",
        "        # 添加特殊标记\n",
        "        self.vocab = ['<PAD>', '<UNK>'] + vocab_items\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "\n",
        "        print(f\"词汇表大小: {len(self.vocab)}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def text_to_sequence(self, text):\n",
        "        \"\"\"文本转换为序列\"\"\"\n",
        "        cleaned_text = self.clean_text(text)\n",
        "        words = cleaned_text.split()\n",
        "\n",
        "        # 转换为索引\n",
        "        indices = []\n",
        "        for word in words[:self.max_seq_length]:\n",
        "            idx = self.word_to_idx.get(word, 1)  # 1 是 <UNK> 的索引\n",
        "            indices.append(idx)\n",
        "\n",
        "        # 填充到固定长度\n",
        "        if len(indices) < self.max_seq_length:\n",
        "            indices.extend([0] * (self.max_seq_length - len(indices)))\n",
        "\n",
        "        return indices[:self.max_seq_length]\n",
        "\n",
        "class ToxicCommentDataset(Dataset):\n",
        "    \"\"\"有毒评论数据集\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, preprocessor):\n",
        "        self.texts = texts\n",
        "        self.labels = labels if labels is not None else [[0]*6]*len(texts)\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        sequence = self.preprocessor.text_to_sequence(text)\n",
        "\n",
        "        # 创建attention mask（非零位置为1）\n",
        "        attention_mask = [1 if token != 0 else 0 for token in sequence]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(sequence, dtype=torch.long),\n",
        "            torch.tensor(attention_mask, dtype=torch.long),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        )\n",
        "\n",
        "def multilabel_accuracy(y_hat, y):\n",
        "    \"\"\"多标签分类准确率\"\"\"\n",
        "    predictions = torch.sigmoid(y_hat) > 0.5\n",
        "    y = y.bool()\n",
        "    label_wise_acc = (predictions == y).float().mean()\n",
        "    return label_wise_acc.item()\n",
        "\n",
        "def train_lstm_epoch(net, train_iter, loss, updater, device, scheduler=None):\n",
        "    \"\"\"\n",
        "    单个epoch训练 - 双向LSTM版本 + 混合精度训练 + 学习率调度\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    metric = Accumulator(3)  # 训练损失总和, 准确数, 样本数\n",
        "\n",
        "    # 使用混合精度训练\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    for _, batch in enumerate(train_iter):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device, non_blocking=True)\n",
        "        attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        # 混合精度前向传播\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "                l = loss(y_hat, labels)\n",
        "        else:\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            l = loss(y_hat, labels)\n",
        "\n",
        "        updater.zero_grad()\n",
        "\n",
        "        # 混合精度反向传播\n",
        "        if scaler is not None:\n",
        "            scaler.scale(l.sum()).backward()\n",
        "            scaler.unscale_(updater)\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            scaler.step(updater)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            l.sum().backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            updater.step()\n",
        "\n",
        "        # 学习率调度（OneCycleLR需要在每个batch后调用）\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])\n",
        "\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "def evaluate_lstm_accuracy(net, data_iter, device):\n",
        "    net.eval()\n",
        "    metric = Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # 使用混合精度推理\n",
        "            if device.type == 'cuda':\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    y_hat = net(input_ids, attention_mask)\n",
        "            else:\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_lstm_model(net, train_iter, test_iter, loss, trainer, num_epochs, devices, scheduler=None):\n",
        "    \"\"\"\n",
        "    完整训练流程 - 双向LSTM版本\n",
        "    \"\"\"\n",
        "    print('training on', devices)\n",
        "\n",
        "    if isinstance(devices, list) and len(devices) > 1:\n",
        "        # 多GPU\n",
        "        net = nn.DataParallel(net, device_ids=devices)\n",
        "\n",
        "    device = devices[0] if isinstance(devices, list) else devices\n",
        "    net = net.to(device)\n",
        "\n",
        "    timer = Timer()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练 - 传递调度器\n",
        "        train_loss, train_acc = train_lstm_epoch(\n",
        "            net, train_iter, loss, trainer, device, scheduler\n",
        "        )\n",
        "\n",
        "        # 验证\n",
        "        test_acc = evaluate_lstm_accuracy(net, test_iter, device)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}: '\n",
        "              f'loss {train_loss:.3f}, '\n",
        "              f'train acc {train_acc:.3f}, '\n",
        "              f'test acc {test_acc:.3f}, '\n",
        "              f'lr {trainer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    print(f'Training completed in {timer.stop():.1f} sec')\n",
        "    print(f'Final: train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
        "\n",
        "def read_toxic_comments_real(data_dir, max_samples=None, is_train=True):\n",
        "    \"\"\"\n",
        "    读取真实的Kaggle Toxic Comment Classification数据\n",
        "    返回格式: (texts, labels, ids)\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        csv_path = os.path.join(data_dir, 'train.csv')\n",
        "        print(f\"读取训练数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "        labels = df[label_columns].values.tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载训练数据: {len(texts)} 条\")\n",
        "        print(f\"标签分布: {dict(zip(label_columns, df[label_columns].sum().tolist()))}\")\n",
        "\n",
        "        return texts, labels, ids\n",
        "    else:\n",
        "        csv_path = os.path.join(data_dir, 'test.csv')\n",
        "        print(f\"读取测试数据: {csv_path}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        texts = df['comment_text'].tolist()\n",
        "        ids = df['id'].tolist()\n",
        "\n",
        "        print(f\"加载测试数据: {len(texts)} 条\")\n",
        "\n",
        "        return texts, None, ids\n",
        "\n",
        "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
        "    \"\"\"\n",
        "    生成Kaggle提交文件\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(\"🔮 生成预测结果...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, _ = batch\n",
        "            input_ids = input_ids.to(device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "            # 使用混合精度推理\n",
        "            if device.type == 'cuda':\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(input_ids, attention_mask)\n",
        "            else:\n",
        "                logits = model(input_ids, attention_mask)\n",
        "\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            predictions.extend(probs)\n",
        "\n",
        "    # 创建提交DataFrame\n",
        "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
        "    })\n",
        "\n",
        "    # 保存提交文件\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"💾 提交文件已保存: {output_path}\")\n",
        "    print(f\"📊 预测统计:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
        "        print(f\"  {col}: 平均概率 {avg_prob:.4f}\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# ============ 主要执行代码 ============\n",
        "print(\"🚀 启动双向LSTM多标签分类训练\")\n",
        "\n",
        "# 数据目录\n",
        "data_dir = 'toxic-comment'\n",
        "\n",
        "# 数据加载\n",
        "print(\"📊 加载真实Kaggle数据...\")\n",
        "\n",
        "# 为了快速训练，限制样本数（可以根据需要调整）\n",
        "train_texts, train_labels, train_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=None, is_train=True\n",
        ")\n",
        "\n",
        "# 创建验证集（从训练数据中分割）\n",
        "val_split = int(len(train_texts) * 0.8)\n",
        "val_texts = train_texts[val_split:]\n",
        "val_labels = train_labels[val_split:]\n",
        "train_texts = train_texts[:val_split]\n",
        "train_labels = train_labels[:val_split]\n",
        "\n",
        "# 读取测试数据\n",
        "test_texts, _, test_ids = read_toxic_comments_real(\n",
        "    data_dir, max_samples=None, is_train=False\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 数据统计:\")\n",
        "print(f\"训练数据: {len(train_texts)} 条\")\n",
        "print(f\"验证数据: {len(val_texts)} 条\")\n",
        "print(f\"测试数据: {len(test_texts)} 条\")\n",
        "\n",
        "# 检查数据质量\n",
        "print(f\"\\n📝 数据样例:\")\n",
        "print(f\"文本长度: {len(train_texts[0])}\")\n",
        "print(f\"前100字符: {train_texts[0][:100]}\")\n",
        "print(f\"标签: {train_labels[0]}\")\n",
        "\n",
        "# 创建文本预处理器和词汇表 - 优化参数以提升速度\n",
        "preprocessor = TextPreprocessor(\n",
        "    max_vocab_size=20000,  # 从50000降到20000\n",
        "    max_seq_length=64,     # 从128降到64\n",
        "    min_freq=3             # 提高最小频率阈值\n",
        ")\n",
        "preprocessor.build_vocab(train_texts)\n",
        "\n",
        "print(f\"\\n🔧 预处理器测试:\")\n",
        "sample_sequence = preprocessor.text_to_sequence(train_texts[0])\n",
        "print(f\"序列长度: {len(sample_sequence)}\")\n",
        "print(f\"非零token数: {sum(1 for x in sample_sequence if x != 0)}\")\n",
        "\n",
        "# 模型参数 - 优化以提升训练速度\n",
        "num_classes = 6  # 6个类别：toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
        "dropout = 0.3\n",
        "# 根据GPU情况自动调整批次大小\n",
        "batch_size = 32 if torch.cuda.is_available() else 16\n",
        "num_steps = 128   # 序列长度（从128降到64）\n",
        "lr = 2e-3        # 提高学习率以加快收敛\n",
        "num_epochs = 3   # 训练轮数\n",
        "\n",
        "# 创建双向LSTM模型 - 优化结构以提升速度\n",
        "net = BiLSTMSentimentClassifier(\n",
        "    vocab_size=len(preprocessor.vocab),\n",
        "    embedding_dim=100,      # 从100降到64\n",
        "    hidden_dim=128,         # 从128降到96\n",
        "    num_layers=2,          # 从2降到1\n",
        "    num_classes=num_classes,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "print(f\"模型参数数量: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "\n",
        "# 创建数据加载器\n",
        "train_dataset = ToxicCommentDataset(train_texts, train_labels, preprocessor)\n",
        "val_dataset = ToxicCommentDataset(val_texts, val_labels, preprocessor)\n",
        "test_dataset = ToxicCommentDataset(test_texts, None, preprocessor)\n",
        "\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                          num_workers=4, pin_memory=True, persistent_workers=True)\n",
        "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                       num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                        num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "# 优化器和学习率调度器\n",
        "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "# 添加学习率调度器以提升训练效果\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    trainer,\n",
        "    max_lr=lr * 5,  # 最大学习率\n",
        "    steps_per_epoch=len(train_iter),\n",
        "    epochs=num_epochs,\n",
        "    pct_start=0.3  # 前30%时间用于升温\n",
        ")\n",
        "\n",
        "# 损失函数 - 多标签分类使用BCEWithLogitsLoss\n",
        "loss = nn.BCEWithLogitsLoss(reduction=\"none\")  # 每个样本每个标签独立计算\n",
        "\n",
        "# 训练\n",
        "print(f\"🔥 使用设备: {device}\")\n",
        "\n",
        "# 开始训练 - 使用学习率调度器\n",
        "train_lstm_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 双向LSTM训练完成!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"✅ 模型: BiLSTM + Attention\")\n",
        "print(f\"✅ 训练样本: {len(train_texts)}\")\n",
        "print(f\"✅ 验证样本: {len(val_texts)}\")\n",
        "print(f\"✅ 测试样本: {len(test_texts)}\")\n",
        "print(\"✅ 支持多标签分类\")\n",
        "print(\"✅ 词汇表大小: {:,}\".format(len(preprocessor.vocab)))\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35KighgnXCUo",
        "outputId": "1ea3559b-923a-4ab4-8c7d-e07aea89a39d"
      },
      "id": "35KighgnXCUo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch版本: 2.8.0+cu126\n",
            "NumPy: 2.0.2\n",
            "🔥 使用设备: cuda\n",
            "🚀 启动双向LSTM多标签分类训练\n",
            "📊 加载真实Kaggle数据...\n",
            "读取训练数据: toxic-comment/train.csv\n",
            "加载训练数据: 159571 条\n",
            "标签分布: {'toxic': 15294, 'severe_toxic': 1595, 'obscene': 8449, 'threat': 478, 'insult': 7877, 'identity_hate': 1405}\n",
            "读取测试数据: toxic-comment/test.csv\n",
            "加载测试数据: 153164 条\n",
            "\n",
            "📊 数据统计:\n",
            "训练数据: 127656 条\n",
            "验证数据: 31915 条\n",
            "测试数据: 153164 条\n",
            "\n",
            "📝 数据样例:\n",
            "文本长度: 264\n",
            "前100字符: Explanation\n",
            "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't \n",
            "标签: [0, 0, 0, 0, 0, 0]\n",
            "📝 构建词汇表...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "处理文本: 100%|██████████| 127656/127656 [00:03<00:00, 33197.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词汇表大小: 20000\n",
            "\n",
            "🔧 预处理器测试:\n",
            "序列长度: 64\n",
            "非零token数: 46\n",
            "模型参数数量: 2,664,711\n",
            "🔥 使用设备: cuda\n",
            "training on cuda\n",
            "Epoch 1: loss 0.855, train acc 0.962, test acc 0.963, lr 0.009944\n",
            "Epoch 2: loss 0.827, train acc 0.963, test acc 0.963, lr 0.004625\n",
            "Epoch 3: loss 0.744, train acc 0.963, test acc 0.963, lr 0.000000\n",
            "Training completed in 174.7 sec\n",
            "Final: train acc 0.963, test acc 0.963\n",
            "\n",
            "============================================================\n",
            "🎉 双向LSTM训练完成!\n",
            "============================================================\n",
            "✅ 模型: BiLSTM + Attention\n",
            "✅ 训练样本: 127656\n",
            "✅ 验证样本: 31915\n",
            "✅ 测试样本: 153164\n",
            "✅ 支持多标签分类\n",
            "✅ 词汇表大小: 20,000\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成提交文件\n",
        "submission_path = os.path.join(data_dir, 'submission.csv')\n",
        "submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)\n",
        "\n",
        "print(\"\\n🎉 双向LSTM训练和预测完成!\")\n",
        "print(f\"✅ 提交文件: {submission_path}\")"
      ],
      "metadata": {
        "id": "mlNsQ60KXIEE"
      },
      "id": "mlNsQ60KXIEE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}