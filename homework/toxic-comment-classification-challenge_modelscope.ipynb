{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0b746c-fc88-41f8-8acd-fbd84fd5798e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  5 13:27:39 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 12.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:08.0 Off |                  Off |\n",
      "| N/A   28C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True,\n",
    "   text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98e4b84-3e49-407a-ab6d-48aaa3ef6802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.13.1+cu116\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp311-cp311-linux_x86_64.whl (1977.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m  \u001b[33m0:02:45\u001b[0m6m0:00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.1+cu116 (from versions: 0.1.6, 0.2.0, 0.15.0+cpu, 0.15.0+cu117, 0.15.0+cu118, 0.15.1, 0.15.1+cpu, 0.15.1+cu117, 0.15.1+cu118, 0.15.2, 0.15.2+cpu, 0.15.2+cu117, 0.15.2+cu118, 0.15.2+rocm5.3, 0.15.2+rocm5.4.2, 0.16.0, 0.16.0+cpu, 0.16.0+cu118, 0.16.0+cu121, 0.16.0+rocm5.5, 0.16.0+rocm5.6, 0.16.1, 0.16.1+cpu, 0.16.1+cu118, 0.16.1+cu121, 0.16.1+rocm5.5, 0.16.1+rocm5.6, 0.16.2, 0.16.2+cpu, 0.16.2+cu118, 0.16.2+cu121, 0.16.2+rocm5.5, 0.16.2+rocm5.6, 0.17.0, 0.17.0+cpu, 0.17.0+cu118, 0.17.0+cu121, 0.17.0+rocm5.6, 0.17.0+rocm5.7, 0.17.1, 0.17.1+cpu, 0.17.1+cu118, 0.17.1+cu121, 0.17.1+rocm5.6, 0.17.1+rocm5.7, 0.17.2, 0.17.2+cpu, 0.17.2+cu118, 0.17.2+cu121, 0.17.2+rocm5.6, 0.17.2+rocm5.7, 0.18.0, 0.18.0+cpu, 0.18.0+cu118, 0.18.0+cu121, 0.18.0+rocm5.7, 0.18.0+rocm6.0, 0.18.1, 0.18.1+cpu, 0.18.1+cu118, 0.18.1+cu121, 0.18.1+rocm5.7, 0.18.1+rocm6.0, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0, 0.22.1, 0.23.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.1+cu116\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    " !pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423391b0-6d36-4094-ba91-acaf04633754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新PyTorch版本: 2.3.1+cu121\n",
      "CUDA可用: True\n",
      "GPU测试: Tesla P100-PCIE-16GB\n",
      "✅ GPU正常工作!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"新PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU测试: {torch.cuda.get_device_name(0)}\")\n",
    "    # 简单GPU测试\n",
    "    x = torch.randn(100, 100).cuda()\n",
    "    y = torch.mm(x, x.t())\n",
    "    print(\"✅ GPU正常工作!\")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd5e9ca-4c36-49b9-86ed-01705ab25059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.8.0+cu128\n",
      "NumPy: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "将BERT集成到您现有的训练代码中 - ModelScope版本\n",
    "使用ModelScope BERT模型进行多标签分类\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from modelscope import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"NumPy: {numpy.__version__}\")\n",
    "\n",
    "\n",
    "# 工具类\n",
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        import time\n",
    "        self.time = time\n",
    "        self.start_time = self.time.time()\n",
    "    def stop(self):\n",
    "        return self.time.time() - self.start_time\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"检测可用GPU\"\"\"\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============ ModelScope BERT模型 ============\n",
    "# 模型定义\n",
    "class BERTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, model_name='AI-ModelScope/distilbert-base-uncased', num_classes=6, dropout=0.1):\n",
    "        super(BERTSentimentClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        print(f\"✅ 加载模型: {model_name}\")\n",
    "        # 冻结策略\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        if hasattr(self.bert, 'transformer'):\n",
    "            for layer in self.bert.transformer.layer[:-1]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "\n",
    "# ============ 修改后的训练函数 ============\n",
    "def train_bert_epoch(net, train_iter, loss, updater, device):\n",
    "    \"\"\"\n",
    "    单个epoch训练 - 添加混合精度训练\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    metric = Accumulator(3)  # 训练损失总和, 准确数, 样本数\n",
    "\n",
    "    # 使用混合精度训练\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "    for _, batch in enumerate(train_iter):\n",
    "        # 解析batch\n",
    "        if len(batch) == 3:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "        else:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 混合精度前向传播\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_hat = net(input_ids, attention_mask)\n",
    "                l = loss(y_hat, labels)\n",
    "        else:\n",
    "            y_hat = net(input_ids, attention_mask)\n",
    "            l = loss(y_hat, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            if scaler is not None:\n",
    "                scaler.scale(l.sum()).backward()\n",
    "                scaler.unscale_(updater)\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "                scaler.step(updater)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                l.sum().backward()\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "                updater.step()\n",
    "\n",
    "        # 统计\n",
    "        with torch.no_grad():\n",
    "            acc = multilabel_accuracy(y_hat, labels)\n",
    "            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])\n",
    "\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def multilabel_accuracy(y_hat, y):\n",
    "    predictions = torch.sigmoid(y_hat) > 0.5\n",
    "    y = y.bool()\n",
    "    label_wise_acc = (predictions == y).float().mean()\n",
    "    return label_wise_acc.item()\n",
    "def train_bert_epoch(net, train_iter, loss, updater, device):\n",
    "    net.train()\n",
    "    metric = Accumulator(3)\n",
    "    for _, batch in enumerate(train_iter):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_hat = net(input_ids, attention_mask)\n",
    "        l = loss(y_hat, labels)\n",
    "        updater.zero_grad()\n",
    "        l.sum().backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(),max_norm=1.0)\n",
    "        updater.step()\n",
    "        with torch.no_grad():\n",
    "            acc = multilabel_accuracy(y_hat, labels)\n",
    "            metric.add(l.sum(), acc * labels.shape[0],labels.shape[0])\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "def evaluate_bert_accuracy(net, data_iter, device):\n",
    "    net.eval()\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_hat = net(input_ids, attention_mask)\n",
    "            acc = multilabel_accuracy(y_hat, labels)\n",
    "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train_bert_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices, scheduler=None):\n",
    "    \"\"\"\n",
    "    完整训练流程\n",
    "    \"\"\"\n",
    "    print('training on', devices)\n",
    "\n",
    "    if isinstance(devices, list) and len(devices) > 1:\n",
    "        # 多GPU\n",
    "        net = nn.DataParallel(net, device_ids=devices)\n",
    "\n",
    "    device = devices[0] if isinstance(devices, list) else devices\n",
    "    net = net.to(device)\n",
    "\n",
    "    timer = Timer()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        train_loss, train_acc = train_bert_epoch(\n",
    "            net, train_iter, loss, trainer, device\n",
    "        )\n",
    "\n",
    "        # 验证\n",
    "        test_acc = evaluate_bert_accuracy(net, test_iter, device)\n",
    "\n",
    "        # 学习率调度\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}: '\n",
    "              f'loss {train_loss:.3f}, '\n",
    "              f'train acc {train_acc:.3f}, '\n",
    "              f'test acc {test_acc:.3f}')\n",
    "\n",
    "    print(f'Training completed in {timer.stop():.1f} sec')\n",
    "    print(f'Final: train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "\n",
    "\n",
    "def read_toxic_comments_real(data_dir, max_samples=None, is_train=True):\n",
    "    \"\"\"\n",
    "    读取真实的Kaggle Toxic Comment Classification数据\n",
    "    返回格式: (texts, labels, ids)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    if is_train:\n",
    "        csv_path = os.path.join(data_dir, 'train.csv')\n",
    "        print(f\"读取训练数据: {csv_path}\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if max_samples:\n",
    "            df = df.head(max_samples)\n",
    "\n",
    "        texts = df['comment_text'].tolist()\n",
    "        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        labels = df[label_columns].values.tolist()\n",
    "        ids = df['id'].tolist()\n",
    "\n",
    "        print(f\"加载训练数据: {len(texts)} 条\")\n",
    "        print(f\"标签分布: {dict(zip(label_columns, df[label_columns].sum().tolist()))}\")\n",
    "\n",
    "        return texts, labels, ids\n",
    "    else:\n",
    "        csv_path = os.path.join(data_dir, 'test.csv')\n",
    "        print(f\"读取测试数据: {csv_path}\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if max_samples:\n",
    "            df = df.head(max_samples)\n",
    "\n",
    "        texts = df['comment_text'].tolist()\n",
    "        ids = df['id'].tolist()\n",
    "\n",
    "        print(f\"加载测试数据: {len(texts)} 条\")\n",
    "\n",
    "        return texts, None, ids\n",
    "\n",
    "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
    "    \"\"\"\n",
    "    生成Kaggle提交文件\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    print(\"🔮 生成预测结果...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, _ = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            # 获取logits并转换为概率\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            predictions.extend(probs)\n",
    "\n",
    "    # 创建提交DataFrame\n",
    "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
    "    })\n",
    "\n",
    "    # 保存提交文件\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"💾 提交文件已保存: {output_path}\")\n",
    "    print(f\"📊 预测统计:\")\n",
    "    for i, col in enumerate(label_columns):\n",
    "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
    "        print(f\"  {col}: 平均概率 {avg_prob:.4f}\")\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "# 数据集\n",
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        if labels is not None:\n",
    "            self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        else:\n",
    "            self.labels = torch.zeros((len(texts), 6),\n",
    "dtype=torch.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return (\n",
    "            encoding['input_ids'].squeeze(),\n",
    "            encoding['attention_mask'].squeeze(),\n",
    "            self.labels[idx]\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949ff486-aa79-4983-a257-8980eeecca4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 测试数据加载...\n",
      "✅ 训练数据读取成功: 10 行\n",
      "✅ 测试数据读取成功: 10 行\n",
      "数据加载完成，没有问题\n"
     ]
    }
   ],
   "source": [
    "  import pandas as pd\n",
    "  import os\n",
    "\n",
    "  print(\"🔄 测试数据加载...\")\n",
    "  data_dir = 'toxic-comment'\n",
    "\n",
    "  try:\n",
    "      # 只读取几行数据\n",
    "      train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'),\n",
    "  nrows=10)\n",
    "      print(f\"✅ 训练数据读取成功: {len(train_df)} 行\")\n",
    "\n",
    "      test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'),\n",
    "  nrows=10)\n",
    "      print(f\"✅ 测试数据读取成功: {len(test_df)} 行\")\n",
    "\n",
    "      print(\"数据加载完成，没有问题\")\n",
    "  except Exception as e:\n",
    "      print(f\"❌ 数据加载失败: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c36f6f-5e4b-4c78-a4d1-0d7bba70f971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/hub/models/AI-ModelScope/bert-base-uncased\n",
      "✅ Tokenizer测试成功\n",
      "编码shape: torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 先尝试简单的tokenizer\n",
    "    from modelscope import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('AI-ModelScope/bert-base-uncased')\n",
    "    # 测试编码\n",
    "    text = \"This is a test\"\n",
    "    encoding = tokenizer(text, max_length=32, truncation=True,\n",
    "padding='max_length', return_tensors='pt')\n",
    "    print(f\"✅ Tokenizer测试成功\")\n",
    "    print(f\"编码shape: {encoding['input_ids'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Tokenizer测试失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0ff64f-9e25-4287-b07c-f6d7e0be5a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 测试基础BERT模型加载...\n",
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/hub/models/AI-ModelScope/distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 21:09:24.727865: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-04 21:09:26.352233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT模型加载成功\n",
      "✅ 模型前向传播成功: torch.Size([1, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "try:\n",
    "    print(\"🔄 测试基础BERT模型加载...\")\n",
    "    from modelscope import AutoModel\n",
    "    # 使用最简单的BERT模型\n",
    "    model = AutoModel.from_pretrained('AI-ModelScope/distilbert-base-uncased')\n",
    "    print(f\"✅ BERT模型加载成功\")\n",
    "    # 测试前向传播\n",
    "    input_ids = torch.randint(0, 1000, (1, 32))\n",
    "    attention_mask = torch.ones(1, 32)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids,\n",
    "attention_mask=attention_mask)\n",
    "    print(f\"✅ 模型前向传播成功: {outputs.last_hidden_state.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee6ba28-36d4-497c-b8ad-7573966a2c92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 测试完整分类模型创建...\n",
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/hub/models/AI-ModelScope/distilbert-base-uncased\n",
      "✅ 加载模型: AI-ModelScope/distilbert-base-uncased\n",
      "✅ 分类模型创建成功\n",
      "✅ 分类模型前向传播成功: torch.Size([2, 6])\n",
      "📊 输出logits范围: -0.309 ~ 0.298\n",
      "📊 sigmoid概率范围: 0.423 ~ 0.574\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"🔄 测试完整分类模型创建...\")\n",
    "    class BERTSentimentClassifier(nn.Module):\n",
    "        def __init__(self, \n",
    "model_name='AI-ModelScope/distilbert-base-uncased', num_classes=6, \n",
    "dropout=0.1):\n",
    "            super(BERTSentimentClassifier, self).__init__()\n",
    "            from modelscope import AutoModel\n",
    "            self.bert = AutoModel.from_pretrained(model_name)\n",
    "            print(f\"✅ 加载模型: {model_name}\")\n",
    "            # 冻结策略 - 只训练最后1层和分类器\n",
    "            for param in self.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            # DistilBERT结构\n",
    "            if hasattr(self.bert, 'transformer'):\n",
    "                for layer in self.bert.transformer.layer[:-1]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            hidden_size = self.bert.config.hidden_size\n",
    "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids,\n",
    "attention_mask=attention_mask)\n",
    "            pooled_output = outputs.last_hidden_state[:, 0]\n",
    "            return self.classifier(self.dropout(pooled_output))\n",
    "    # 创建模型\n",
    "    net = BERTSentimentClassifier(num_classes=6, dropout=0.1)\n",
    "    print(f\"✅ 分类模型创建成功\")\n",
    "    # 测试前向传播\n",
    "    input_ids = torch.randint(0, 1000, (2, 32))\n",
    "    attention_mask = torch.ones(2, 32)\n",
    "    with torch.no_grad():\n",
    "        logits = net(input_ids, attention_mask)\n",
    "    print(f\"✅ 分类模型前向传播成功: {logits.shape}\")\n",
    "    print(f\"📊 输出logits范围: {logits.min():.3f} ~ {logits.max():.3f}\")\n",
    "    # 测试sigmoid概率\n",
    "    probs = torch.sigmoid(logits)\n",
    "    print(f\"📊 sigmoid概率范围: {probs.min():.3f} ~ {probs.max():.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型创建失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dffdf39b-f6e2-4766-be64-bf55b416ce2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 测试训练循环组件...\n",
      "📊 模拟数据:\n",
      "  logits shape: torch.Size([2, 6])\n",
      "  labels shape: torch.Size([2, 6])\n",
      "  labels 示例: tensor([0., 0., 0., 0., 0., 0.])\n",
      "✅ 损失计算成功: torch.Size([2, 6]), 平均损失: 0.9190\n",
      "✅ 准确率计算成功: 0.3333\n",
      "✅ 优化器创建成功\n",
      "✅ 训练步骤成功, 损失: 0.6747\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"🔄 测试训练循环组件...\")\n",
    "    from torch.optim import AdamW\n",
    "    # 测试损失函数\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    # 创建模拟数据\n",
    "    batch_size = 2\n",
    "    num_classes = 6\n",
    "    logits = torch.randn(batch_size, num_classes)\n",
    "    labels = torch.randint(0, 2, (batch_size, num_classes)).float()\n",
    "    print(f\"📊 模拟数据:\")\n",
    "    print(f\"  logits shape: {logits.shape}\")\n",
    "    print(f\"  labels shape: {labels.shape}\")\n",
    "    print(f\"  labels 示例: {labels[0]}\")\n",
    "    # 测试损失计算\n",
    "    loss = loss_fn(logits, labels)\n",
    "    print(f\"✅ 损失计算成功: {loss.shape}, 平均损失: {loss.mean():.4f}\")\n",
    "    # 测试准确率计算\n",
    "    def multilabel_accuracy(y_hat, y):\n",
    "        predictions = torch.sigmoid(y_hat) > 0.5\n",
    "        y = y.bool()\n",
    "        label_wise_acc = (predictions == y).float().mean()\n",
    "        return label_wise_acc.item()\n",
    "    acc = multilabel_accuracy(logits, labels)\n",
    "    print(f\"✅ 准确率计算成功: {acc:.4f}\")\n",
    "    # 测试优化器\n",
    "    optimizer = AdamW([\n",
    "        {'params': net.classifier.parameters(), 'lr': 1e-3},\n",
    "        {'params': [p for p in net.bert.parameters() if p.requires_grad], 'lr': 1e-4}\n",
    "    ], weight_decay=0.01)\n",
    "    print(f\"✅ 优化器创建成功\")\n",
    "    # 测试一步训练\n",
    "    optimizer.zero_grad()\n",
    "    logits = net(input_ids, attention_mask)\n",
    "    loss = loss_fn(logits, labels).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"✅ 训练步骤成功, 损失: {loss.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 训练循环测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebb8cb-8423-4d08-bb4a-be538679bd60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 启动ModelScope BERT多标签分类训练\n",
      "📊 加载真实Kaggle数据...\n",
      "读取训练数据: toxic-comment/train.csv\n",
      "加载训练数据: 1000 条\n",
      "标签分布: {'toxic': 105, 'severe_toxic': 9, 'obscene': 49, 'threat': 4, 'insult': 54, 'identity_hate': 9}\n",
      "读取测试数据: toxic-comment/test.csv\n",
      "加载测试数据: 1000 条\n",
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/hub/models/AI-ModelScope/bert-base-uncased\n",
      "\n",
      "📊 数据统计:\n",
      "训练数据: 800 条\n",
      "验证数据: 200 条\n",
      "测试数据: 1000 条\n",
      "\n",
      "📝 数据样例:\n",
      "文本长度: 264\n",
      "前100字符: Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't \n",
      "标签: [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "🔧 Tokenizer测试:\n",
      "input_ids shape: torch.Size([1, 128])\n",
      "attention_mask shape: torch.Size([1, 128])\n",
      "实际token数: 68\n",
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/hub/models/AI-ModelScope/distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 21:32:58.257984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-04 21:32:59.915545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 加载模型: AI-ModelScope/distilbert-base-uncased\n",
      "🔥 使用设备: cuda\n",
      "training on cuda\n"
     ]
    }
   ],
   "source": [
    "# ============ 主要执行代码 ============\n",
    "print(\"🚀 启动ModelScope BERT多标签分类训练\")\n",
    "# 数据目录\n",
    "data_dir = 'toxic-comment'\n",
    "# 数据加载\n",
    "print(\"📊 加载真实Kaggle数据...\")\n",
    "# 为了快速训练，限制样本数（可以根据需要调整）\n",
    "train_texts, train_labels, train_ids = read_toxic_comments_real(\n",
    "    data_dir, max_samples=1000, is_train=True\n",
    ")\n",
    "# 创建验证集（从训练数据中分割）\n",
    "val_split = int(len(train_texts) * 0.8)\n",
    "val_texts = train_texts[val_split:]\n",
    "val_labels = train_labels[val_split:]\n",
    "train_texts = train_texts[:val_split]\n",
    "train_labels = train_labels[:val_split]\n",
    "# 读取测试数据\n",
    "test_texts, _, test_ids = read_toxic_comments_real(\n",
    "    data_dir, max_samples=1000, is_train=False\n",
    ")\n",
    "# 初始化tokenizer - 使用指定的ModelScope BERT模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('AI-ModelScope/bert-base-uncased')\n",
    "print(f\"\\n📊 数据统计:\")\n",
    "print(f\"训练数据: {len(train_texts)} 条\")\n",
    "print(f\"验证数据: {len(val_texts)} 条\")\n",
    "print(f\"测试数据: {len(test_texts)} 条\")\n",
    "# 检查数据质量\n",
    "print(f\"\\n📝 数据样例:\")\n",
    "print(f\"文本长度: {len(train_texts[0])}\")\n",
    "print(f\"前100字符: {train_texts[0][:100]}\")\n",
    "print(f\"标签: {train_labels[0]}\")\n",
    "# 检查tokenizer\n",
    "sample_encoding = tokenizer(train_texts[0], max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "print(f\"\\n🔧 Tokenizer测试:\")\n",
    "print(f\"input_ids shape: {sample_encoding['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {sample_encoding['attention_mask'].shape}\")\n",
    "print(f\"实际token数: {sample_encoding['attention_mask'].sum()}\")\n",
    "# 模型参数\n",
    "num_classes = 6  # 6个类别：toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "dropout = 0.1\n",
    "batch_size = 16  # 适配在线环境\n",
    "num_steps = 32  # 序列长度\n",
    "lr = 1e-4  # 学习率\n",
    "num_epochs = 3  # 增加到3个epoch获得更好效果\n",
    " # 使用ModelScope上实际存在的BERT模型\n",
    "model_name = 'AI-ModelScope/distilbert-base-uncased'\n",
    "# 创建模型\n",
    "net = BERTSentimentClassifier(\n",
    "    model_name=model_name,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = BERTDataset(train_texts, train_labels, tokenizer, max_length=num_steps)\n",
    "val_dataset = BERTDataset(val_texts, val_labels, tokenizer, max_length=num_steps)\n",
    "test_dataset = BERTDataset(test_texts, None, tokenizer, max_length=num_steps)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                         num_workers=0, pin_memory=False)\n",
    "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                       num_workers=0, pin_memory=False)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                        num_workers=0, pin_memory=False)\n",
    "# 优化器 - 只训练最后一层和分类器\n",
    "trainer = AdamW([\n",
    "    {'params': [p for name, p in net.named_parameters() if 'classifier' in name], 'lr': lr * 10},  # 分类层\n",
    "    {'params': [p for name, p in net.named_parameters() if 'classifier' not in name and p.requires_grad], 'lr': lr}  # BERT层\n",
    "], weight_decay=0.01)\n",
    "# 损失函数 - 多标签分类使用BCEWithLogitsLoss\n",
    "loss = nn.BCEWithLogitsLoss(reduction=\"none\")  # 每个样本每个标签独立计算\n",
    "# 训练\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔥 使用设备: {device}\")\n",
    "# 开始训练\n",
    "train_bert_ch13(net, train_iter, val_iter, loss, trainer, num_epochs, device, None)\n",
    "# 生成提交文件\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📤 生成Kaggle提交文件...\")\n",
    "print(\"=\"*60)\n",
    "submission_path = 'toxic_comment/submission.csv'\n",
    "submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 ModelScope BERT训练和预测完成!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✅ 模型:{model_name}\")\n",
    "print(f\"✅ 训练样本: {len(train_texts)}\")\n",
    "print(f\"✅ 验证样本: {len(val_texts)}\")\n",
    "print(f\"✅ 测试样本: {len(test_texts)}\")\n",
    "print(f\"✅ 提交文件: {submission_path}\")\n",
    "print(\"✅ 支持多标签分类\")\n",
    "print(\"✅ 混合精度训练\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7095947c-6091-4e21-b3c8-d7de82ec18bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
